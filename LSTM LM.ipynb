{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Models\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "Dataset: Lord of the Rings Movie Dialogues\n",
    "Source: Kaggle – Paul Timothy Mooney\n",
    "Theme: Fantasy / LOTR\n",
    "Content: Character dialogues from the LOTR trilogy\n",
    "Why suitable:\n",
    "\n",
    "Text-rich, narrative dialogue\n",
    "\n",
    "Sequential structure → good for language modeling\n",
    "\n",
    "Public, well-known repository\n",
    "Dataset source:\n",
    "Paul Timothy Mooney, “Lord of the Rings Dataset”, Kaggle.\n",
    "https://www.kaggle.com/datasets/paultimothymooney/lord-of-the-rings-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'char', 'dialog', 'movie'], dtype='object')\n",
      "   Unnamed: 0     char                                             dialog  \\\n",
      "0           0   DEAGOL  Oh Smeagol Ive got one! , Ive got a fish Smeag...   \n",
      "1           1  SMEAGOL     Pull it in! Go on, go on, go on, pull it in!     \n",
      "2           2   DEAGOL                                           Arrghh!    \n",
      "3           3  SMEAGOL                                          Deagol!     \n",
      "4           4  SMEAGOL                                          Deagol!     \n",
      "\n",
      "                     movie  \n",
      "0  The Return of the King   \n",
      "1  The Return of the King   \n",
      "2  The Return of the King   \n",
      "3  The Return of the King   \n",
      "4  The Return of the King   \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"lotr_scripts.csv\")\n",
    "df.head()\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialog lines: 1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     Oh Smeagol Ive got one! , Ive got a fish Smeag...\n",
       "1          Pull it in! Go on, go on, go on, pull it in!\n",
       "6                          Give us that! Deagol my love\n",
       "8           Because' , it's my birthday and I wants it.\n",
       "12    'Murderer' they called us. They cursed us and ...\n",
       "Name: dialog, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop useless column\n",
    "dataset = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Keep dialog only\n",
    "texts = dataset[\"dialog\"].dropna().astype(str)\n",
    "\n",
    "# Basic cleanup\n",
    "texts = texts.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Remove very short lines (noise)\n",
    "texts = texts[texts.str.len() > 20]\n",
    "\n",
    "print(\"Number of dialog lines:\", len(texts))\n",
    "texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1315\n",
      "Valid: 164\n",
      "Test : 165\n"
     ]
    }
   ],
   "source": [
    "# Ensure Python list (not pandas Series)\n",
    "texts = texts.tolist()\n",
    "\n",
    "# Shuffle deterministically\n",
    "random.shuffle(texts)\n",
    "\n",
    "n = len(texts)\n",
    "train_end = int(0.8 * n)\n",
    "valid_end = int(0.9 * n)\n",
    "\n",
    "train_lines = texts[:train_end]\n",
    "valid_lines = texts[train_end:valid_end]\n",
    "test_lines  = texts[valid_end:]\n",
    "\n",
    "print(\"Train:\", len(train_lines))\n",
    "print(\"Valid:\", len(valid_lines))\n",
    "print(\"Test :\", len(test_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts), type(train_lines[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look Mr Frodo, a doorway! We're almost there!\n",
      "Merry! Merry? Wake up\n",
      "It's going to the Great Eye, along with everything else.\n",
      "Hoi! You get back here! Wait till I get this through you! , Get out of my fields! You'll know the devil if I catch up with you!\n",
      "He's up to something. , All right then, keep your secrets!\n",
      "Now come the days of the King. May they be blessed.\n",
      "They would be small.Only children to your eyes.\n",
      "and some cabbages and those few bags of potatoes that we lifted last week. And the mushr\n"
     ]
    }
   ],
   "source": [
    "train_raw = \"\\n\".join(train_lines)\n",
    "valid_raw = \"\\n\".join(valid_lines)\n",
    "test_raw  = \"\\n\".join(test_lines)\n",
    "\n",
    "print(train_raw[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text = \"\\n\".join(train_lines)\n",
    "\n",
    "\n",
    "# print(raw_text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         char                                             dialog  \\\n",
      "0      DEAGOL  Oh Smeagol Ive got one! , Ive got a fish Smeag...   \n",
      "1     SMEAGOL     Pull it in! Go on, go on, go on, pull it in!     \n",
      "2      DEAGOL                                           Arrghh!    \n",
      "3     SMEAGOL                                          Deagol!     \n",
      "4     SMEAGOL                                          Deagol!     \n",
      "...       ...                                                ...   \n",
      "2385   PIPPIN                                            Merry!    \n",
      "2386  ARAGORN                                            Merry!    \n",
      "2387    MERRY  He's always followed me everywhere I went sinc...   \n",
      "2388  ARAGORN  One thing I've learnt about Hobbits: They are ...   \n",
      "2389    MERRY                     Foolhardy maybe. He's a Took!    \n",
      "\n",
      "                        movie  \n",
      "0     The Return of the King   \n",
      "1     The Return of the King   \n",
      "2     The Return of the King   \n",
      "3     The Return of the King   \n",
      "4     The Return of the King   \n",
      "...                       ...  \n",
      "2385  The Return of the King   \n",
      "2386  The Return of the King   \n",
      "2387  The Return of the King   \n",
      "2388  The Return of the King   \n",
      "2389  The Return of the King   \n",
      "\n",
      "[2390 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing \n",
    "The raw text is first converted to lowercase and tokenized using a basic English tokenizer that removes non-alphanumeric characters.\n",
    "An <eos> token is appended to mark sentence boundaries.\n",
    "A vocabulary is constructed by keeping tokens that appear at least three times in the training set, while rare tokens are mapped to <unk>.\n",
    "Each token is then converted to a numerical index to form input sequences.\n",
    "The dataset is split into training, validation, and test sets to evaluate generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'mr', 'frodo', 'a', 'doorway', \"we're\", 'almost', 'there', 'merry', 'merry', 'wake', 'up', \"it's\", 'going', 'to', 'the', 'great', 'eye', 'along', 'with']\n"
     ]
    }
   ],
   "source": [
    "def basic_english_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9']+\", \" \", text)\n",
    "    return [t for t in text.strip().split() if t]\n",
    "\n",
    "tokenizer = basic_english_tokenizer\n",
    "\n",
    "train_tokens = tokenizer(train_raw)\n",
    "valid_tokens = tokenizer(valid_raw)\n",
    "test_tokens  = tokenizer(test_raw)\n",
    "\n",
    "print(train_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_dataset['train'][223]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n",
      "Look Mr Frodo, a doorway! We're almost there!\n",
      "['look', 'mr', 'frodo', 'a', 'doorway', \"we're\", 'almost', 'there', 'merry', 'merry', 'wake', 'up', \"it's\", 'going', 'to', 'the', 'great', 'eye', 'along', 'with']\n"
     ]
    }
   ],
   "source": [
    "print(type(train_lines), type(train_tokens))\n",
    "print(train_lines[0])\n",
    "print(train_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 18772\n",
      "Valid tokens: 2346\n",
      "Test tokens : 2497\n"
     ]
    }
   ],
   "source": [
    "print(\"Train tokens:\", len(train_tokens))\n",
    "print(\"Valid tokens:\", len(valid_tokens))\n",
    "print(\"Test tokens :\", len(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.itos = list(tokens)\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        self.default_index = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.stoi[token]\n",
    "        return self.default_index\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "\n",
    "    def set_default_index(self, idx):\n",
    "        self.default_index = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 884\n",
      "['<unk>', '<eos>', 'look', 'mr', 'frodo', 'a', \"we're\", 'almost', 'there', 'merry']\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(train_tokens)\n",
    "\n",
    "vocab_tokens = [\"<unk>\", \"<eos>\"]\n",
    "for tok, freq in counts.items():\n",
    "    if freq >= 3:\n",
    "        vocab_tokens.append(tok)\n",
    "\n",
    "vocab = SimpleVocab(vocab_tokens)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Vocab size:\", len(vocab.get_itos()))\n",
    "print(vocab.get_itos()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 293]) torch.Size([64, 36]) torch.Size([64, 39])\n"
     ]
    }
   ],
   "source": [
    "def make_data(tokens, vocab, batch_size):\n",
    "    # Numeric tokens + <eos>\n",
    "    ids = [vocab[t] for t in tokens] + [vocab[\"<eos>\"]]\n",
    "    data = torch.LongTensor(ids)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    # Drop remainder & reshape\n",
    "    data = data[:num_batches * batch_size]\n",
    "    return data.view(batch_size, -1)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_data = make_data(train_tokens, vocab, batch_size)\n",
    "valid_data = make_data(valid_tokens, vocab, batch_size)\n",
    "test_data  = make_data(test_tokens,  vocab, batch_size)\n",
    "\n",
    "print(train_data.shape, valid_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SimpleVocab object at 0x7e1509b369c0>\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', 'look', 'mr', 'frodo', 'a', \"we're\", 'almost', 'there', 'merry']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(dataset, vocab, batch_size):\n",
    "#     data = []\n",
    "#     for example in dataset:\n",
    "#         if example['tokens']:\n",
    "#             tokens = example['tokens'] + ['<eos>']\n",
    "#             tokens = [vocab[token] for token in tokens]\n",
    "#             data.extend(tokens)\n",
    "\n",
    "#     data = torch.LongTensor(data)\n",
    "#     num_batches = data.shape[0] // batch_size\n",
    "#     data = data[:num_batches * batch_size]\n",
    "#     return data.view(batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "# test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i used a flat token stream with manual batching and truncated BPTT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 293])\n",
      "torch.Size([64, 36])\n",
      "torch.Size([64, 39])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling \n",
    "We implement a word-level LSTM language model consisting of an embedding layer, a multi-layer LSTM, and a linear output layer.\n",
    "The model is trained using cross-entropy loss with teacher forcing, where the target sequence is shifted by one token relative to the input.\n",
    "Truncated backpropagation through time is used to manage long sequences, and gradient clipping is applied to stabilize training.\n",
    "Model performance is evaluated using perplexity on validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/LM.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return h0, c0\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        h, c = hidden\n",
    "        return h.detach(), c.detach()\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        # src: [batch_size, seq_len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # output: [batch_size, seq_len, hid_dim]\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        # prediction: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_itos())\n",
    "emb_dim = 512             # 400 in the paper\n",
    "hid_dim = 512                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 884 <class 'int'>\n",
      "emb_dim: 512 <class 'int'>\n",
      "hid_dim: 512 <class 'int'>\n",
      "num_layers: 2 <class 'int'>\n",
      "dropout_rate: 0.65 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab_size:\", vocab_size, type(vocab_size))\n",
    "print(\"emb_dim:\", emb_dim, type(emb_dim))\n",
    "print(\"hid_dim:\", hid_dim, type(hid_dim))\n",
    "print(\"num_layers:\", num_layers, type(num_layers))\n",
    "print(\"dropout_rate:\", dropout_rate, type(dropout_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Trainable parameters: 5,108,596\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    hid_dim=hid_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout_rate=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {num_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "\n",
    "#     num_tokens = data.size(1)\n",
    "#     num_tokens = num_tokens - (num_tokens - 1) % seq_len\n",
    "#     data = data[:, :num_tokens]\n",
    "\n",
    "#     hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "#     for idx in tqdm(range(0, num_tokens - 1, seq_len), desc=\"Training\", leave=False):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         hidden = model.detach_hidden(hidden)\n",
    "\n",
    "#         src, target = get_batch(data, seq_len, idx)\n",
    "#         src, target = src.to(device), target.to(device)\n",
    "\n",
    "#         output, hidden = model(src, hidden)\n",
    "\n",
    "#         output = output.reshape(-1, output.size(-1))\n",
    "#         target = target.reshape(-1)\n",
    "\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item() * seq_len\n",
    "\n",
    "#     return epoch_loss / num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    num_tokens = data.size(1)\n",
    "    num_tokens = num_tokens - (num_tokens - 1) % seq_len\n",
    "    data = data[:, :num_tokens]\n",
    "\n",
    "    hidden = model.init_hidden(data.size(0), device)  # ✅ FIX\n",
    "\n",
    "    for idx in tqdm(range(0, num_tokens - 1, seq_len), desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "\n",
    "        output, hidden = model(src, hidden)\n",
    "\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    num_tokens = data.size(1)\n",
    "    num_tokens = num_tokens - (num_tokens - 1) % seq_len\n",
    "    data = data[:, :num_tokens]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_tokens - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "\n",
    "            output, hidden = model(src, hidden)\n",
    "\n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 613.2276437064652\n",
      "Valid perplexity: 242.4408163514481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "seq_len = 35\n",
    "clip = 0.25\n",
    "batch_size = 64\n",
    "train_loss = train_epoch(\n",
    "    model, train_data, optimizer, criterion,\n",
    "    batch_size, seq_len, clip, device\n",
    ")\n",
    "\n",
    "valid_loss = evaluate(\n",
    "    model, valid_data, criterion,\n",
    "    batch_size, seq_len, device\n",
    ")\n",
    "\n",
    "print(\"Train perplexity:\", math.exp(train_loss))\n",
    "print(\"Valid perplexity:\", math.exp(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  Train Perplexity: 250.090\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "  Train Perplexity: 225.368\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "  Train Perplexity: 215.428\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "  Train Perplexity: 211.162\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "  Train Perplexity: 210.012\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "  Train Perplexity: 210.499\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "  Train Perplexity: 210.057\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "  Train Perplexity: 209.105\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "  Train Perplexity: 208.194\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "  Train Perplexity: 209.127\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "  Train Perplexity: 208.977\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "  Train Perplexity: 208.557\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "  Train Perplexity: 207.540\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "  Train Perplexity: 208.425\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "  Train Perplexity: 209.056\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "  Train Perplexity: 207.804\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "  Train Perplexity: 209.822\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "  Train Perplexity: 209.378\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "  Train Perplexity: 208.744\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "  Train Perplexity: 208.817\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "  Train Perplexity: 209.181\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "  Train Perplexity: 209.051\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "  Train Perplexity: 208.637\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "  Train Perplexity: 209.930\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "  Train Perplexity: 208.640\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "  Train Perplexity: 210.012\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "  Train Perplexity: 209.165\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "  Train Perplexity: 210.060\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "  Train Perplexity: 210.642\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "  Train Perplexity: 209.231\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "  Train Perplexity: 208.584\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "  Train Perplexity: 207.861\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "  Train Perplexity: 208.898\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "  Train Perplexity: 208.560\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "  Train Perplexity: 209.020\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "  Train Perplexity: 209.409\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "  Train Perplexity: 207.969\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "  Train Perplexity: 208.658\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "  Train Perplexity: 208.222\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "  Train Perplexity: 208.878\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "  Train Perplexity: 208.672\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "  Train Perplexity: 209.856\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "  Train Perplexity: 209.303\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "  Train Perplexity: 208.280\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "  Train Perplexity: 208.546\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "  Train Perplexity: 209.144\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "  Train Perplexity: 208.875\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "  Train Perplexity: 208.934\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "  Train Perplexity: 208.805\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "  Train Perplexity: 208.721\n",
      "  Valid Perplexity: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, patience=0\n",
    ")\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        model, train_data, optimizer, criterion,\n",
    "        batch_size, seq_len, clip, device\n",
    "    )\n",
    "\n",
    "    valid_loss = evaluate(\n",
    "        model, valid_data, criterion,\n",
    "        batch_size, seq_len, device\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best-val-lstm_lm.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"  Train Perplexity: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"  Valid Perplexity: {math.exp(valid_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 1.000\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation split is very small, leading to optimistic perplexity.\n",
    "Test Perplexity: 1.000 is suspiciously low, due Small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            src = torch.LongTensor([[indices[-1]]]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
    "\n",
    "            for _ in range(10):  # avoid infinite unk loop\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                if next_token != vocab['<unk>']:\n",
    "                    break\n",
    "\n",
    "            if next_token == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(next_token)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "bilbo baggins is rain day it you to sleep to the of to the of but you no to the we the and the it i have a the the you to i\n",
      "\n",
      "0.7\n",
      "bilbo baggins is rain day dangerous down counsel to sleep which world with the will to the of but says no be the we the and the it in must nice think of\n",
      "\n",
      "0.75\n",
      "bilbo baggins is rain beer day dangerous down counsel here sleep which world with the will to the of but says no be the we kings and the it ha must nice think\n",
      "\n",
      "0.8\n",
      "bilbo baggins is rain beer day dangerous down counsel here sleep which world with the will to the of but says no be he we kings and the it ha must nice think\n",
      "\n",
      "1.0\n",
      "bilbo baggins is rain beer day dangerous pippin down counsel here sleep which world with the will to the of but says no be he we kings and the it looks ha must\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Bilbo baggins is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower temperatures sharpen the softmax distribution, favoring high-probability tokens and producing more deterministic text. Higher temperatures flatten the distribution, increasing randomness and lexical diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment implemented a word-level LSTM language model trained using truncated backpropagation through time. The model successfully learns local syntactic patterns and produces coherent short sequences, demonstrating that recurrent neural networks can model sequential dependencies in natural language.\n",
    "\n",
    "However, several limitations are observed. First, the model operates at the word level with a fixed vocabulary, which leads to reliance on the <unk> token and limits its ability to generalize to unseen words. Second, the LSTM processes sequences sequentially, making training and inference less efficient compared to modern parallel architectures. Long-range dependencies may also be weakened despite the use of multiple layers, as information must pass through many recurrent steps.\n",
    "\n",
    "Additionally, the model lacks attention mechanisms, which restricts its ability to selectively focus on relevant parts of the context. As a result, generated text may exhibit repetitive patterns or reduced global coherence, especially for longer sequences. The relatively small dataset further limits linguistic diversity and encourages memorization rather than robust generalization.\n",
    "\n",
    "In modern natural language processing systems, Transformer-based models have largely replaced LSTMs due to their ability to model long-range dependencies more effectively and to leverage parallel computation. Nonetheless, LSTM-based language models remain valuable for educational purposes, as they provide clear insight into sequence modeling, hidden state dynamics, and training techniques such as gradient clipping and truncated backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
